{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "gray = (167/255, 168/255, 170/255, 1)\n",
    "red = (179/255, 27/255, 27/255, 1)\n",
    "blue = (0,47/255, 108/255,1)\n",
    "markersize=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = \"'threeDots'\" # note the single quotes to match the evaluation_results json\n",
    "# combinedOutputs is the path from the root directory to the JL1-VAE models. \n",
    "# This path works for models trained directly with\n",
    "# ./experimentScripts/train_jlonevae/train_threeDots.bash\n",
    "# and evaluated with\n",
    "# ./experimentScripts/evaluate_jlonevae/evaluate_threeDots.bash\n",
    "# which stores models and evaluations in directories like:\n",
    "# ./trainedModels/defaultConv_lone_beta4_0000_ica0_1000_lat10_batch64_lr0_0001_anneal100000/20210604-014949/representation\n",
    "combinedOutputs = \"trainedModels\"\n",
    "# If unzipping pre-trained models you may need to change this path.\n",
    "# For example, if you download \"combinedOutputsThreeDotsReplicationRunFinal.zip\"\n",
    "# and unzip it into the directory \"threeDotsDownload\",\n",
    "# then you should use filepath:\n",
    "# combinedOutputs = \"threeDotsDownload/combinedOutputs/threeDots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gather evaluation results\n",
    "evaluation_result_template = \"{}/metrics/{}/results/aggregate/evaluation.json\"\n",
    "experiment_output_path = f\"../../{combinedOutputs}\"\n",
    "for metric, metvalname in [(\"local_mig\",\"evaluation_results.local_discrete_migs_samples\"), \n",
    "               (\"local_modularity\",\"evaluation_results.local_modularity_scores_samples\")]:\n",
    "    for latdim in [\"10\"]:\n",
    "      f = plt.figure(figsize=(10,8))\n",
    "      for bind, beta in enumerate([\"4\"]):\n",
    "        experiment_names = [f\"defaultConv_lone_beta{beta}_0000_ica0_1000_lat{latdim}_batch64_lr0_00??_anneal100000\",\n",
    "                            f\"defaultConv_lone_beta{beta}_0000_ica0_0000_lat{latdim}_batch64_lr0_00??_anneal100000\"\n",
    "                           ]\n",
    "        data = []\n",
    "        for enind, experiment_name in enumerate(experiment_names):\n",
    "            if enind==1:\n",
    "                color=blue\n",
    "                marker=\"+\"\n",
    "                label=\"beta-VAE\"\n",
    "            elif enind==0:\n",
    "                color=red\n",
    "                marker=\"x\"\n",
    "                label=\"JL1-VAE\"\n",
    "            evaluation_filepaths = glob.glob(f\"{experiment_output_path}/{experiment_name}/*/metrics/{metric}*/results/aggregate/evaluation.json\")\n",
    "            for ind, filename in enumerate(evaluation_filepaths):\n",
    "                #print(filename)\n",
    "                evaluation_results = json.loads(\n",
    "                        open(filename, \"r\").read())\n",
    "                if evaluation_results['evaluation_config.dataset.name'] != datasetName:\n",
    "                    print(f\"Skipping model evaluated on {evaluation_results['evaluation_config.dataset.name']}\")\n",
    "                    continue\n",
    "                locality = float(evaluation_results[\"evaluation_config.local_sample_factors.locality_proportion\"])\n",
    "                met_samps = evaluation_results[metvalname]\n",
    "                data.append((label, locality, np.mean(met_samps)))\n",
    "                if bind == 0 and ind == 0 and enind < 2:\n",
    "                    plt.plot(locality, np.mean(met_samps), markerfacecolor=(1, 1, 1, 0), markeredgecolor=color, markeredgewidth=2, marker=marker,label=label, markersize=markersize,linestyle=\"None\" )\n",
    "                else:\n",
    "                    plt.plot(locality, np.mean(met_samps), markerfacecolor=(1, 1, 1, 0), markeredgecolor=color, markeredgewidth=2, marker=marker, markersize=markersize,linestyle=\"None\" )\n",
    "        plt.ylabel(metric.replace(\"_\",\" \").replace(\"mig\",\"MIG\"))\n",
    "        plt.xlabel(\"Locality radius (fraction of factor range)\");\n",
    "        plt.xlim(-0.05,1.05)\n",
    "\n",
    "      import scipy.stats\n",
    "      avgBetaMetVals = []\n",
    "      avgLirjMetVals = []\n",
    "      allLocalities = [0.1, 0.2, 0.3, 0.6, 1.0]\n",
    "      for desiredLocality in allLocalities:\n",
    "        betametvals = [metval for (label, locality, metval) in data if label == \"beta-VAE\" and locality == desiredLocality]\n",
    "        lirjmetvals = [metval for (label, locality, metval) in data if label == \"JL1-VAE\" and locality == desiredLocality]\n",
    "        avgBetaMetVals.append(np.mean(betametvals))\n",
    "        avgLirjMetVals.append(np.mean(lirjmetvals))\n",
    "        print(scipy.stats.ttest_ind(betametvals, lirjmetvals))\n",
    "      plt.plot(allLocalities, avgLirjMetVals, color=red)\n",
    "      plt.plot(allLocalities, avgBetaMetVals, color=blue)\n",
    "      plt.legend()\n",
    "      plt.tight_layout()\n",
    "      f.savefig(f\"threeDots_{metric}_varyingRho.png\")\n",
    "      plt.show();plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
